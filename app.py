#!/usr/bin/env python3
"""
Deepfake Audio Detection Web Interface
=====================================

Flask web application for deepfake audio detection with file upload,
real-time detection, and report generation.

Author: ML Engineer
Date: 24 Haziran 2025
"""

import os
import uuid
import librosa
import numpy as np
import tensorflow as tf
from datetime import datetime
from flask import Flask, render_template, request, jsonify, send_file, redirect, url_for
from werkzeug.utils import secure_filename
import pickle
import matplotlib
matplotlib.use('Agg')  # For non-interactive backend
import matplotlib.pyplot as plt
import io
import base64
try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.lib import colors
    REPORTLAB_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  ReportLab not available. PDF reports will be disabled.")
    REPORTLAB_AVAILABLE = False

# Initialize Flask app
app = Flask(__name__)
app.config['SECRET_KEY'] = 'deepfake-detection-2025'
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# Create uploads directory
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# Global variables for model and scaler
model = None
scaler = None

def load_model_and_scaler():
    """Load the trained model and scaler."""
    global model, scaler
    
    try:
        print("Loading trained model...")
        model_path = "results/best_advanced_cnn_model.h5"
        model = tf.keras.models.load_model(model_path)
        print("Model loaded successfully!")
        
        print("Loading feature scaler...")
        scaler_path = "results/feature_scaler.pkl"
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        print("‚úÖ Scaler loaded successfully!")
        
        return True
    except Exception as e:
        print(f"‚ùå Error loading model or scaler: {e}")
        return False

def allowed_file(filename):
    """Check if uploaded file is allowed."""
    ALLOWED_EXTENSIONS = {'wav', 'mp3', 'flac', 'm4a', 'ogg'}
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_audio_features(file_path):
    """Extract mel-spectrogram from audio file."""
    try:
        # Load audio file
        audio, sr = librosa.load(file_path, sr=16000, duration=2.0)
        
        # Ensure audio is exactly 2 seconds
        target_length = 4 * 16000
        if len(audio) < target_length:
            # Pad with zeros if too short
            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')
        else:
            # Truncate if too long
            audio = audio[:target_length]
        
        # Extract mel-spectrogram
        mel_spectrogram = librosa.feature.melspectrogram(
            y=audio,
            sr=sr,
            n_mels=128,
            n_fft=2048,
            hop_length=512,
            fmax=8000
        )
        
        # Convert to decibels
        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
        
        # Ensure shape is (128, 63)
        if mel_spectrogram_db.shape[1] != 63:
            if mel_spectrogram_db.shape[1] > 63:
                mel_spectrogram_db = mel_spectrogram_db[:, :63]
            else:
                # Pad with minimum values
                pad_width = 63 - mel_spectrogram_db.shape[1]
                mel_spectrogram_db = np.pad(
                    mel_spectrogram_db, 
                    ((0, 0), (0, pad_width)), 
                    mode='constant', 
                    constant_values=mel_spectrogram_db.min()
                )
        
        # Normalize
        mel_spectrogram_db = (mel_spectrogram_db - mel_spectrogram_db.min()) / \
                           (mel_spectrogram_db.max() - mel_spectrogram_db.min())
        
        # Add channel dimension for CNN
        mel_spectrogram_db = mel_spectrogram_db[..., np.newaxis]
        
        return mel_spectrogram_db, audio, sr
    
    except Exception as e:
        print(f"Error extracting features: {e}")
        return None, None, None

def predict_deepfake(spectrogram):
    """Predict if audio is deepfake using the trained model."""
    try:
        # Add batch dimension
        spectrogram_batch = np.expand_dims(spectrogram, axis=0)
        
        # Predict
        prediction_proba = model.predict(spectrogram_batch, verbose=0)[0][0]
        
        # Use optimal threshold
        optimal_threshold = 0.0001
        is_fake = prediction_proba > optimal_threshold
        
        # Calculate confidence
        if is_fake:
            confidence = min(prediction_proba * 10000, 99.9)  # Scale up from very small values
        else:
            confidence = min((1 - prediction_proba) * 100, 99.9)
        
        return {
            'is_fake': bool(is_fake),
            'confidence': float(confidence),
            'raw_probability': float(prediction_proba),
            'threshold_used': optimal_threshold
        }
    
    except Exception as e:
        print(f"Error in prediction: {e}")
        return None

def create_spectrogram_plot(spectrogram, audio, sr):
    """Create spectrogram visualization."""
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # Plot waveform
    time = np.linspace(0, len(audio) / sr, len(audio))
    ax1.plot(time, audio)
    ax1.set_title('Audio Waveform')
    ax1.set_xlabel('Time (seconds)')
    ax1.set_ylabel('Amplitude')
    ax1.grid(True, alpha=0.3)
    
    # Plot spectrogram
    spectrogram_2d = spectrogram.squeeze()  # Remove channel dimension
    im = ax2.imshow(spectrogram_2d, aspect='auto', origin='lower', cmap='viridis')
    ax2.set_title('Mel-Spectrogram')
    ax2.set_xlabel('Time Frames')
    ax2.set_ylabel('Mel Frequency Bins')
    plt.colorbar(im, ax=ax2)
    
    plt.tight_layout()
    
    # Convert to base64 for web display
    img_buffer = io.BytesIO()
    plt.savefig(img_buffer, format='png', dpi=100, bbox_inches='tight')
    img_buffer.seek(0)
    plot_data = base64.b64encode(img_buffer.getvalue()).decode()
    plt.close()
    
    return plot_data

def generate_pdf_report(result_data):
    """Generate PDF report of the detection results."""
    filename = f"deepfake_detection_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    
    # Create PDF document
    doc = SimpleDocTemplate(filepath, pagesize=A4)
    styles = getSampleStyleSheet()
    story = []
    
    # Title
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=24,
        spaceAfter=30,
        alignment=1,  # Center alignment
        textColor=colors.darkblue
    )
    story.append(Paragraph("DEEPFAKE AUDIO DETECTION REPORT", title_style))
    story.append(Spacer(1, 20))
    
    # Report info
    info_style = ParagraphStyle(
        'InfoStyle',
        parent=styles['Normal'],
        fontSize=12,
        spaceAfter=10
    )
    
    report_date = datetime.now().strftime('%d/%m/%Y %H:%M:%S')
    story.append(Paragraph(f"<b>Report Generated:</b> {report_date}", info_style))
    story.append(Paragraph(f"<b>File Name:</b> {result_data['filename']}", info_style))
    story.append(Paragraph(f"<b>Analysis Model:</b> Advanced CNN (90% Accuracy)", info_style))
    story.append(Spacer(1, 20))
    
    # Results section
    result_style = ParagraphStyle(
        'ResultStyle',
        parent=styles['Heading2'],
        fontSize=16,
        spaceAfter=15,
        textColor=colors.darkgreen if not result_data['is_fake'] else colors.darkred
    )
    
    result_text = "REAL AUDIO DETECTED" if not result_data['is_fake'] else "FAKE AUDIO DETECTED"
    story.append(Paragraph(result_text, result_style))
    
    # Detailed results table
    data = [
        ['Metric', 'Value'],
        ['Classification', 'Real Audio' if not result_data['is_fake'] else 'Deepfake Audio'],
        ['Confidence Level', f"{result_data['confidence']:.1f}%"],
        ['Raw Probability', f"{result_data['raw_probability']:.6f}"],
        ['Threshold Used', f"{result_data['threshold_used']:.6f}"],
        ['Model Accuracy', '90.0%'],
        ['Model Type', 'Advanced CNN'],
    ]
    
    table = Table(data, colWidths=[2*inch, 2*inch])
    table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 12),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    
    story.append(table)
    story.append(Spacer(1, 30))
    
    # Technical details
    tech_title = ParagraphStyle(
        'TechTitle',
        parent=styles['Heading2'],
        fontSize=14,
        spaceAfter=10
    )
    story.append(Paragraph("Technical Details", tech_title))
    
    tech_text = f"""
    <b>Analysis Method:</b> The audio file was processed using advanced machine learning techniques.
    The system extracts mel-spectrogram features and analyzes them using a trained Convolutional Neural Network (CNN).
    <br/><br/>
    <b>Model Performance:</b> Our Advanced CNN model achieves 90% accuracy with an F1-score of 89.8% on test data.
    The model uses an optimized threshold of 0.0001 for classification decisions.
    <br/><br/>
    <b>Confidence Interpretation:</b> The confidence level indicates how certain the model is about its prediction.
    Higher confidence values suggest more reliable results.
    <br/><br/>
    <b>Note:</b> This analysis is based on the current state-of-the-art deepfake detection technology.
    For critical applications, we recommend additional verification methods.
    """
    
    story.append(Paragraph(tech_text, styles['Normal']))
    story.append(Spacer(1, 20))
    
    # Footer
    footer_style = ParagraphStyle(
        'FooterStyle',
        parent=styles['Normal'],
        fontSize=10,
        alignment=1,
        textColor=colors.grey
    )
    story.append(Paragraph("Generated by Deepfake Audio Detection System v1.0", footer_style))
    
    # Build PDF
    doc.build(story)
    
    return filename

@app.route('/')
def index():
    """Main page with file upload."""
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    """Handle file upload and process audio."""
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'error': 'Invalid file format. Please upload WAV, MP3, FLAC, M4A, or OGG files.'}), 400
    
    try:
        # Generate unique filename
        file_id = str(uuid.uuid4())
        filename = secure_filename(file.filename)
        file_ext = filename.rsplit('.', 1)[1].lower()
        unique_filename = f"{file_id}.{file_ext}"
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
        
        # Save uploaded file
        file.save(filepath)
        
        # Extract features
        spectrogram, audio, sr = extract_audio_features(filepath)
        if spectrogram is None:
            return jsonify({'error': 'Error processing audio file'}), 500
        
        # Predict
        result = predict_deepfake(spectrogram)
        if result is None:
            return jsonify({'error': 'Error during prediction'}), 500
        
        # Create visualization
        plot_data = create_spectrogram_plot(spectrogram, audio, sr)
        
        # Prepare response
        response_data = {
            'file_id': file_id,
            'filename': filename,
            'is_fake': result['is_fake'],
            'confidence': result['confidence'],
            'raw_probability': result['raw_probability'],
            'threshold_used': result['threshold_used'],
            'plot_data': plot_data,
            'file_size': os.path.getsize(filepath),
            'duration': len(audio) / sr
        }
        
        # Clean up uploaded file
        os.remove(filepath)
        
        return jsonify(response_data)
    
    except Exception as e:
        print(f"Error processing file: {e}")
        return jsonify({'error': f'Error processing file: {str(e)}'}), 500

@app.route('/generate_report', methods=['POST'])
def generate_report():
    """Generate and download PDF report."""
    try:
        data = request.json
        
        # Generate PDF report
        pdf_filename = generate_pdf_report(data)
        pdf_path = os.path.join(app.config['UPLOAD_FOLDER'], pdf_filename)
        
        return jsonify({
            'success': True,
            'report_url': f'/download_report/{pdf_filename}'
        })
    
    except Exception as e:
        print(f"Error generating report: {e}")
        return jsonify({'error': f'Error generating report: {str(e)}'}), 500

@app.route('/download_report/<filename>')
def download_report(filename):
    """Download generated PDF report."""
    try:
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        return send_file(filepath, as_attachment=True, download_name=filename)
    except Exception as e:
        print(f"Error downloading report: {e}")
        return "Report not found", 404

if __name__ == '__main__':
    print("üöÄ Starting Deepfake Audio Detection Web Interface...")
    
    # Load model and scaler
    if load_model_and_scaler():
        print("‚úÖ System ready!")
        print("üåê Starting web server...")
        app.run(debug=True, host='0.0.0.0', port=8080)
    else:
        print("‚ùå Failed to load model. Please check the model files.")
